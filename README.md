### LLMS

### Evaluation of Instruction Following

**1.Single-Turn**

1.Measuring massive multitask language understanding. 2020

2.Agieval: A human-centric benchmark for evaluating foundation models. 2023

3.Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022

4.Alpacaeval: An automatic evaluator of instruction-following models. 2023

5.Can large language models understand real-world complex instructions? 2023

6.Instruction-following evaluation for large language models. 2023


**2.Multi-turn**

first evaluation benchmark for multi-turn instruction following.
